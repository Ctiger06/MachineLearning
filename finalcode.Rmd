Data Mining Course Project - Predicting Activity Categories
========================================================
## Author: Christie Entwistle
## Date: 1/22/16

###Summary
The goal of this project is to predict weightlifting activity categories A through E, where each category corresponds to a particular manner in which the weightlifting is conducted (Class A: exactly according to specification, Class B: throwing elbows to the front, Class C: lifting dumbbell only halfway, Class D: lowering dumbbell only halfway, Class E: throwing hips to the front). The training set used for this exercise comes from http://groupware.les.inf.puc-rio.br/har and contains measured features coming from sensors placed on various parts of the body and the equipment used.

Several predictive models were considered. Accuracy of cross-validated results was used to decide between models, and accuracy on a hold-out set was used estimate the out of sample error of the final predictive model. Additionally, the run-time of the models was considered in choosing a model, as there is often a tradeoff between accuracy and computational speed (even though parallel processing was used for bagged models). In the end, a bagged CART model was chosen due to its relatively high accuracy and relatively quick computational speed.

###Data Preprocessing
```{r, echo = FALSE}
setwd("C:/Users/c.entwistle/Desktop")
setwd("Coursera/Machine Learning/Project/data")
training<-read.csv("pml-training.csv",  na.strings = c("NA", "" ))
testing<-read.csv("pml-testing.csv",  na.strings = c("NA", ""))
dfresw<-dget("dfresw")

```
Note: For brevity, the R code for preprocessing is not shown.

The training set and the test set for the project were read in. The training set dimensions (rows, columns) were (`r dim(training)`) and the testing set dimensions were (`r dim(testing)`). In order to properly assess the out of sample error, a training and test set within the training set were created with the caret package.

```{r, echo = FALSE, message = FALSE}
library(caret)
inTrain<-createDataPartition(y = training$classe, p = .75, list = FALSE)
train<-training[inTrain,]
test<-training[-inTrain,]
```
Examining our new training set, it appears that the variables are either fully populated with no NA values, or they are very sparsely populated with mostly NA values. Because of this pattern in the data, we can reduce the feature space by ignoring any features containing NA values. Then looking at the remaining variables with the summary function, we see some more columns that probably should be omitted (e.g. time stamp, window, etc). Of course the same preprocessing steps must be applied to the test set. 

```{r, echo = FALSE}
train<-train[,which(sapply(train,function(x) sum(1*is.na(x))==0))]
train<-train[,-c(1,3,4,5,6,7)]
test<-test[,which(sapply(test,function(x) sum(1*is.na(x))==0))]
test<-test[,-c(1,3,4,5,6,7)]
```
Below are the remaining features and class variable that will be considered in the model (set "train"). 

```{r, echo = FALSE}
names(train)
```
There are still many features in the model, so it is of interest whether some of them are highly correlated. Below is a table showing highly correlated variables. 

```{r, echo = FALSE}
library(reshape2)
trainingnum<-train[,which(sapply(train,class)%in% c("integer", "numeric"))]
cormtrx<-cor(trainingnum)
cormtrx<-lower.tri(cormtrx)*cormtrx
cortbl<-melt(cormtrx)
highcor<-cortbl[which((abs(cortbl$value)>.85)&(abs(cortbl$value)<1.0)),]
names(highcor) = c("Variable1", "Variable2", "Correlation")
highcor
```
It appears that measurements taken on the belt are highly correlated. Additionally measurements taken with a gyroscope tend to be correlated. Because of this pattern, PCA was run on two subsets of variables - those that contain "belt" measurements and those that contain "gyroscope" measurements. Since the PC's created for these two variables can be associated with those respective groups, we do not lose all interpretability. Running PCA at a 90% variance threshold separately on these subgroups, we look at plots of the two PC's for these groups to see how much separation they give:

```{r, echo = FALSE}
typeColor<-ifelse(train$classe=="A",1,ifelse(train$classe=="B",2,
            ifelse(train$classe=="C", 3, ifelse(train$classe == "D", 4,
            5))))


beltnames<-grep("belt",names(train))
gyrosnames<-grep("gyros", names(train[,-beltnames]))
preProcPCAbelt<-preProcess(train[,beltnames], method = "pca", thresh = .90)
preProcPCAgyros<-preProcess(train[,gyrosnames], method = "pca", thresh = .90)
trainPCbelt<-predict(preProcPCAbelt,train[,beltnames])
names(trainPCbelt)<-paste(names(trainPCbelt),"belt", sep = "_")
plot(trainPCbelt[,c("PC1_belt")],trainPCbelt[,c("PC2_belt")], col = typeColor, main = "First Two Belt PC's")
  #Some good separation


trainPCgyros<-predict(preProcPCAgyros,train[,gyrosnames])
names(trainPCgyros)<-paste(names(trainPCgyros),"gyros", sep = "_")
plot(trainPCgyros[,c("PC1_gyros")],trainPCgyros[,c("PC2_gyros")], col = typeColor, main = "First Two Gyros PC's")
  #Not as good separation
```
It appears from the plot that there could be some good separation of the classe variable from the first and second PC's of the belt PC's, but it does not appear that the same goes for the gyros PC's. Therefore we will only include the PC's for the belt variable. We will replace the belt fields with their PC's and add back in all the other original fields.

Printed below are the names of the trainPC dataframe. The number of columns has been reduced from 54 to 45. Later we will see if this improves the modeling on the classe variable or not.

```{r, echo = FALSE, warning = FALSE, message = FALSE}
nonbeltgyrosdf<-train[,-c(beltnames)]
trainPC<-cbind(trainPCbelt,nonbeltgyrosdf)
names(trainPC)

library(caret)
library(e1071)
library(C50)
library(ipred)
library(plyr)
library(doParallel)
    registerDoParallel(cores = 5)

set.seed(1000)
trainsamp<-train[sample(nrow(train),3000),]
trainPCsamp<-trainPC[sample(nrow(trainPC),3000),]

#Code below used for initially running all the models, but instead of rerunning this code repeatedly in the #.Rmd file I saved the output using dput() and have previously read it back in

#source("../code/final code/trainsampleresults.R")
#dfresw<-samplemodelresults(trainsamp, trainPCsamp)


g<-ggplot(data = dfresw, aes(x = time, y = Accuracy, label = Model))
g<-g + geom_point(col = "blue") + geom_text(aes(label = Model), cex = 3, 
                                         hjust = -.1, vjust = -.5)
g<-g + scale_x_continuous(limits = c(0, 3.0)) + ggtitle("Model Performance on Training Sample")
g + xlab("Time (minutes)") + ylab("Accuracy (10 Fold CV)")

```

Remember this is only assessed on a sample of 3000 observations in the training set. We will train the bagged CART on the entire training set, with 10 fold cross validation. Our selection is based on the plot which shows us that the bagged CART has very high accuracy and also doesn't take too long (none of the models took an extremely long time to complete) for the training sample. One has to apply the prediction function to an independent test set in order to get true out of sample error value, so we will use our test set to do so.

The code for the final model is displayed below.

```{r, echo = TRUE}




start.time <- Sys.time()

modFit <- train(classe~ .,data=train,
                method="treebag", trControl=trainControl(method = "cv", number = 10))

end.time <- Sys.time()
time.taken <- difftime(end.time, start.time, units = "mins")
 

predMod<-predict(modFit, test)
cm<-confusionMatrix(predMod, test$classe)

```

The out of sample accuracy appears to be `r round(cm$overall[1],3)` as compared to  `r round(modFit$results[2],3)`, the 10 fold crossvalidated accuracy on whole training set. 



